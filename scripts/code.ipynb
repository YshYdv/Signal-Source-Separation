{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K5usPI1BrOP"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d kumarvishal88/ml-projectdatet\n",
        "!unzip ml-projectdatet.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ALTERNATE\n",
        "! wget -q -O 'kaggle.json' 'https://drive.google.com/uc?export=download&id=1dtGtms-_JU1ZatceSnzc8mCY1ia9X9um'\n",
        "! pip install kaggle -q\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download kumarvishal88/ml-projectdatet\n",
        "! unzip -o ml-projectdatet\n",
        "! rm -r ml-projectdatet.zip"
      ],
      "metadata": {
        "id": "teoAmvYLhQbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eTNBG4MPioo"
      },
      "source": [
        "# Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv53v10ZCiYV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import soundfile as sf\n",
        "import random\n",
        "\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "from IPython.display import Audio\n",
        "\n",
        "from scipy.stats import skew, kurtosis, iqr\n",
        "from scipy.signal import iirnotch, butter, lfilter\n",
        "\n",
        "from sklearn.decomposition import PCA, FastICA, NMF\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import librosa.effects\n",
        "import librosa.feature\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKJleo-ePayf"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6ggawHmDgpo"
      },
      "outputs": [],
      "source": [
        "directory = \"SignalSourceDataset\"\n",
        "audio_data = []\n",
        "\n",
        "for audio_files in os.listdir(directory):\n",
        "    audio_file = os.path.join(directory, audio_files)\n",
        "    audio, sr = librosa.load(audio_file)\n",
        "    audio_data.append((audio, sr, audio_files))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkuPxOG8R_Eb"
      },
      "source": [
        "# Waveform - Amplitude vs Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSIbFtW9SC5S"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(audio_data):\n",
        "    librosa.display.waveshow(audio, sr=sr, ax=axs[i])\n",
        "    axs[i].set_title(f\"Waveform\\n{audio_files}\")\n",
        "    axs[i].set_xlabel(\"Time\")\n",
        "    axs[i].set_ylabel(\"Amplitude\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The displayed waveforms represent audio amplitudes over time for five different tracks. Variations\n",
        "in waveform patterns suggest distinct audio characteristics and dynamics for each track, indicating\n",
        "diverse musical compositions."
      ],
      "metadata": {
        "id": "S_lBH1g4l9Uf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N581__CeSVKe"
      },
      "source": [
        "# Spectrogram - Frequency vs Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LnmaBToSPQt"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(audio_data):\n",
        "    duration=10\n",
        "    n = random.randint(0, len(audio) - sr * duration)\n",
        "    debi_audio = librosa.amplitude_to_db(librosa.stft(audio[n:n + sr*duration]), ref=np.max)\n",
        "\n",
        "    img = librosa.display.specshow(debi_audio, sr=sr, x_axis=\"time\", y_axis=\"log\", ax=axs[i])\n",
        "    axs[i].set_title(f\"Spectrogram\\n{audio_files}\")\n",
        "\n",
        "fig.colorbar(img, ax=axs, format='%+2.0f dB', orientation='vertical')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The spectrograms depict frequency distributions over time. Each demonstrates unique spectral\n",
        "patterns, indicating varied frequency components and intensities. These differences suggest diverse\n",
        "musical elements and soundscapes across the tracks"
      ],
      "metadata": {
        "id": "CpFRuMnfmX3Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1jIbCViSg1O"
      },
      "source": [
        "# Amplitude Histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXVRqnccShML"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(audio_data):\n",
        "    amplitude_values = np.abs(librosa.effects.preemphasis(audio))\n",
        "    axs[i].hist(amplitude_values, bins=100, color='blue', alpha=0.7)\n",
        "    axs[i].set_title(f\"Amplitude Histogram\\n{audio_files}\")\n",
        "    axs[i].set_xlabel(\"Amplitude\")\n",
        "    axs[i].set_ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The amplitude histograms for the five audio tracks show the distribution of sound levels. Each\n",
        "histogram presents a rapid decline in frequency as amplitude increases, indicating that louder sound\n",
        "levels are less frequent in these tracks"
      ],
      "metadata": {
        "id": "uBJQCQv1mZ0T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS37eelkZjIV"
      },
      "source": [
        "# Chromagram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbROgH6WZmGp"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(audio_data):\n",
        "    duration=10\n",
        "    n = random.randint(0, len(audio) - sr * duration)\n",
        "    chroma = librosa.feature.chroma_stft(y=audio[n : n + sr*duration], sr=sr)\n",
        "    img = librosa.display.specshow(chroma, y_axis='chroma', x_axis='time', ax=axs[i])\n",
        "    axs[i].set_title(f\"Chromagram\\n{audio_files}\")\n",
        "\n",
        "fig.colorbar(img, ax=axs, orientation='vertical')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chromagrams display the pitch intensity over time. Each track exhibits varied pitch distributions, with multiple tracks showing frequent transitions among pitches, suggesting complex musical compositions."
      ],
      "metadata": {
        "id": "uhtg53Hd2gRo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBFz9FXjZph9"
      },
      "source": [
        "# Power Spectral Density"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgJUPcjgZw99"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(audio_data):\n",
        "    axs[i].psd(audio, NFFT=1024, Fs=sr)\n",
        "    axs[i].set_title(f\"PSD\\n{audio_files}\")\n",
        "    axs[i].set_xlabel('Frequency (Hz)')\n",
        "    axs[i].set_ylabel('Power/Frequency (dB/Hz)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Power Spectral Density (PSD) graphs display the power distribution across frequencies. All\n",
        "tracks exhibit a decline in power as frequency increases, with notable variations in intensity and\n",
        "frequency bands among them"
      ],
      "metadata": {
        "id": "-u9ykYSX2reT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPKDlaanc0Uj"
      },
      "source": [
        "# Tempogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I5ImN25c4mm"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(audio_data):\n",
        "    duration=10\n",
        "    n = random.randint(0, len(audio) - sr * duration)\n",
        "    tempo, _ = librosa.beat.beat_track(y=audio[n : n + sr*duration], sr=sr)\n",
        "    tempogram = librosa.feature.tempogram(y=audio[n : n + sr*duration], sr=sr)\n",
        "\n",
        "    img = librosa.display.specshow(tempogram, sr=sr, hop_length=512, x_axis='time', y_axis='tempo', ax=axs[i])\n",
        "    axs[i].axhline(tempo, color='w', linestyle='--', alpha=0.7)\n",
        "    axs[i].set_title(f'Tempogram with Estimated Tempo\\n{audio_files}')\n",
        "\n",
        "fig.colorbar(img, ax=axs, format='%2.1f BPM', orientation='vertical')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tempograms depict the estimated tempos of five audio tracks over time. While all tracks showcase rhythmic structures, variations exist in tempo stability and intensity. \"James May\" and \"Clara Berry\" seem consistent with clear tempo lines, whereas other samples exhibit more varied tempo shifts."
      ],
      "metadata": {
        "id": "hJvGO0sFkHtG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpcCzCSZTdnj"
      },
      "source": [
        "# Basic Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ17RpWPTdE5"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "columns = [\"Author\", \"Song Name\", \"Mean\", \"Median\", \"Standard Deviation\", \"Max\", \"Min\", \"Skew\", \"Kurtosis\", \"Inter Quartile Range\",\n",
        "           \"Spectral Centroid\", \"Spectral Bandwidth\", \"Zero Crossing Rate\"]\n",
        "\n",
        "for audio, sr, file_name in audio_data:\n",
        "    author, song_name = file_name.rsplit('.', 1)[0].split(' - ', 1)\n",
        "\n",
        "    envelope = np.abs(librosa.effects.preemphasis(audio, coef=0.95))\n",
        "    mean_amplitude = np.mean(envelope)\n",
        "    median_amplitude = np.median(envelope)\n",
        "    std_deviation_amplitude = np.std(envelope)\n",
        "    max_amplitude = np.max(envelope)\n",
        "    min_amplitude = np.min(envelope)\n",
        "    skewness = skew(envelope)\n",
        "    kurt = kurtosis(envelope)\n",
        "    iqr_value = iqr(envelope)\n",
        "\n",
        "    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=envelope, sr=sr))\n",
        "    spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=envelope, sr=sr))\n",
        "    zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(envelope))\n",
        "\n",
        "    data.append([author, song_name[:-4], mean_amplitude, median_amplitude, std_deviation_amplitude, max_amplitude, min_amplitude,\n",
        "                 skewness, kurt, iqr_value, spectral_centroid, spectral_bandwidth, zero_crossing_rate])\n",
        "\n",
        "stats_df = pd.DataFrame(data, columns=columns)\n",
        "print(stats_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCzNzAtqXc-d"
      },
      "source": [
        "# Pitch Contour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMjgw_beXcnZ"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 5, figsize=(20, 6))\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(audio_data):\n",
        "    pitch, magnitude = librosa.piptrack(y=audio, sr=sr)\n",
        "    est_pitch = [max(pitch[:, frame]) for frame in range(pitch.shape[1])]\n",
        "    est_pitch = np.array(est_pitch)\n",
        "\n",
        "    axs[i].plot(librosa.times_like(est_pitch), est_pitch, label=\"Estimated Pitch in Hz\")\n",
        "    axs[i].set_ylabel(\"Pitch in Hz\")\n",
        "    axs[i].set_xlabel(\"Time in sec\")\n",
        "    axs[i].set_title(f\"Pitch Contour\\n{audio_files}\")\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m25pVQQ-20zs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqUKSd6TYHuI"
      },
      "source": [
        "# #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3L14Mb9rYIVu"
      },
      "outputs": [],
      "source": [
        "mfccs_data = []\n",
        "\n",
        "for audio, sr, audio_files in audio_data:\n",
        "    duration=10\n",
        "    n = random.randint(0, len(audio) - sr * duration)\n",
        "    mfccs = librosa.feature.mfcc(y=audio[n: n + sr*duration], sr=sr)\n",
        "    mfccs_data.append((mfccs, audio_files))\n",
        "\n",
        "fig, axs = plt.subplots(1, 5, figsize=(20, 6))\n",
        "\n",
        "for i, (mfccs, audio_files) in enumerate(mfccs_data):\n",
        "    img = librosa.display.specshow(mfccs, x_axis='time', ax=axs[i])\n",
        "    axs[i].set_title(f'MFCCs\\n{audio_files}')\n",
        "\n",
        "fig.colorbar(img, ax=axs, format='%+2.0f dB', orientation='vertical')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30w5iDbPbDlQ"
      },
      "source": [
        "# Pitch Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5Z3RCpZbGu9"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(audio_data):\n",
        "    f0, voice_flag, voice_prob = librosa.pyin(audio, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
        "    times = librosa.times_like(f0)\n",
        "\n",
        "    axs[i].plot(times, f0, label='F0 (fundamental frequency)', color='b')\n",
        "    axs[i].set_title(audio_files)\n",
        "    axs[i].set_xlabel('Time (s)')\n",
        "    axs[i].set_ylabel('Frequency (Hz)')\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWG7COoPbb4B"
      },
      "source": [
        "# Harmonic Percussive Source Seperation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLQhXikKcHXv"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(3, 5, figsize=(20, 12))\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(audio_data):\n",
        "\n",
        "    duration=10\n",
        "    n = random.randint(0, len(audio) - sr * duration)\n",
        "\n",
        "    harmonic, percussive = librosa.effects.hpss(audio[n : n + sr*duration])\n",
        "    librosa.display.waveshow(audio[n: n + sr*duration], sr=sr, ax=axs[0, i])\n",
        "    axs[0, i].set_title(f\"Original\\n{audio_files}\")\n",
        "\n",
        "    librosa.display.waveshow(harmonic, sr=sr, color='pink', ax=axs[1, i])\n",
        "    axs[1, i].set_title(f\"Harmonic\\n{audio_files}\")\n",
        "\n",
        "    librosa.display.waveshow(percussive, sr=sr, color='green', ax=axs[2, i])\n",
        "    axs[2, i].set_title(f\"Percussive\\n{audio_files}\")\n",
        "\n",
        "for ax in axs.flat:\n",
        "    ax.label_outer()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGYRK7dwUhGf"
      },
      "source": [
        "# Foreground and Background Separation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILkwEO5JICYI"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(7, 5, figsize=(20, 12))\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(audio_data):\n",
        "    # finding short time fourier transform\n",
        "\n",
        "    duration=30\n",
        "    n = random.randint(0, len(audio) - sr * duration)\n",
        "\n",
        "    x = audio[n:n + sr*duration]\n",
        "\n",
        "    stft=librosa.stft((x))\n",
        "    # separate magnitude and phase\n",
        "    mag,phase=librosa.magphase(stft)\n",
        "    # For denoising, each point is replaced by its nearest neighbours using cosine distance between the medians\n",
        "    filtered_audio=librosa.decompose.nn_filter(mag,aggregate=np.median,metric='cosine',width=int(librosa.time_to_frames(2,sr=sr)))\n",
        "    filtered_audio=np.minimum(filtered_audio,mag)\n",
        "    fore=librosa.util.softmask(mag-filtered_audio,10*filtered_audio,power=2)\n",
        "    back=librosa.util.softmask(filtered_audio,10*(mag-filtered_audio),power=2)\n",
        "    foreground=fore*mag\n",
        "    foreground_audio=librosa.istft(foreground)\n",
        "    background=back*mag\n",
        "    background_audio=librosa.istft(background)\n",
        "\n",
        "    librosa.display.waveshow(x, sr=sr, ax=axs[0, i])\n",
        "    axs[0, i].set_title(f\"Waveform Original\\n{audio_files}\")\n",
        "\n",
        "    librosa.display.waveshow(foreground_audio, sr=sr, color='pink', ax=axs[1, i])\n",
        "    axs[1, i].set_title(f\"Waveform Foreground\\n{audio_files}\")\n",
        "\n",
        "    librosa.display.waveshow(background_audio, sr=sr, color='green', ax=axs[2, i])\n",
        "    axs[2, i].set_title(f\"Waveform Background\\n{audio_files}\")\n",
        "\n",
        "    pitch,magnitude=librosa.piptrack(y=foreground_audio,sr=sr)\n",
        "    est_pitch=[]\n",
        "    for u in range(pitch.shape[1]):\n",
        "      maxi=0\n",
        "      for a in pitch[:,u]:\n",
        "        # print(type(a))\n",
        "        if isinstance(a,np.ndarray):\n",
        "          maxi=max(maxi,max(a))\n",
        "        else:\n",
        "          maxi=max(maxi,a)\n",
        "      est_pitch.append(maxi)\n",
        "    est_pitch=np.array(est_pitch)\n",
        "\n",
        "    axs[3,i].plot(librosa.times_like(est_pitch),est_pitch,label=\"Estimated Pitch in Hz\")\n",
        "    axs[3,i].set_ylabel(\"Pitch in Hz\")\n",
        "    axs[3,i].set_xlabel(\"Time in sec\")\n",
        "    axs[3,i].set_title(f\"Pitch Contour Foreground\\n{audio_files}\")\n",
        "\n",
        "    pitch,magnitude=librosa.piptrack(y=background_audio,sr=sr)\n",
        "    est_pitch=[]\n",
        "    for u in range(pitch.shape[1]):\n",
        "      maxi=0\n",
        "      for a in pitch[:,u]:\n",
        "        if isinstance(a,np.ndarray):\n",
        "          maxi=max(maxi,max(a))\n",
        "        else:\n",
        "          maxi=max(maxi,a)\n",
        "      est_pitch.append(maxi)\n",
        "    est_pitch=np.array(est_pitch)\n",
        "\n",
        "    axs[4,i].plot(librosa.times_like(est_pitch),est_pitch,label=\"Estimated Pitch in Hz\")\n",
        "    axs[4,i].set_ylabel(\"Pitch in Hz\")\n",
        "    axs[4,i].set_xlabel(\"Time in sec\")\n",
        "    axs[4,i].set_title(f\"Pitch Contour Background\\n{audio_files}\")\n",
        "\n",
        "\n",
        "    D=np.abs(foreground)\n",
        "    amp=np.mean(D,axis=0)\n",
        "    phase_diff=np.angle(D[:,1:])-np.angle(D[:,:-1])\n",
        "    inst_freq=np.unwrap(phase_diff)/(2*np.pi)*sr\n",
        "\n",
        "    librosa.display.waveshow(foreground,alpha=0.5, sr=sr, ax=axs[5, i])\n",
        "    axs[5, i].set_title(f\"Tremolos Foreground\\n{audio_files}\")\n",
        "    axs[5, i].plot(np.arange(len(amp))*sr/len(amp),amp,color='r')\n",
        "\n",
        "    D=np.abs(background)\n",
        "    amp=np.mean(D,axis=0)\n",
        "    phase_diff=np.angle(D[:,1:])-np.angle(D[:,:-1])\n",
        "    inst_freq=np.unwrap(phase_diff)/(2*np.pi)*sr\n",
        "\n",
        "    librosa.display.waveshow(background,alpha=0.5, sr=sr, ax=axs[6, i])\n",
        "    axs[6, i].set_title(f\"Tremolos Background\\n{audio_files}\")\n",
        "    axs[6, i].plot(np.arange(len(amp))*sr/len(amp),amp,color='r')\n",
        "\n",
        "\n",
        "\n",
        "    sf.write(audio_files+'foreground.wav', foreground_audio, sr)\n",
        "    sf.write(audio_files+'background.wav', background_audio, sr)\n",
        "\n",
        "for ax in axs.flat:\n",
        "    ax.label_outer()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Foreground and background separation:"
      ],
      "metadata": {
        "id": "qJPau0DDFCFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir=\"SignalSourceDataset\"\n",
        "for i, (audio, sr, audio_files) in enumerate(audio_data):\n",
        "  sf.write(audio_files+'audio.wav', audio, sr)\n",
        "  print(audio_files)\n",
        "  display(Audio(audio_files+'audio.wav'))\n",
        "  display(Audio(audio_files+'foreground.wav'))\n",
        "  display(Audio(audio_files+'background.wav'))"
      ],
      "metadata": {
        "id": "j2qE-eCFPieC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background and Foreground Separation Method 2"
      ],
      "metadata": {
        "id": "lU-lyD0OdihG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = 'SignalSourceDataset'\n",
        "for aud_file in os.listdir(dir):\n",
        "    if aud_file[:-3] == 'mp4':\n",
        "        clip = VideoFileClip(os.path.join(dir, aud_file))\n",
        "        clip.audio.write_audiofile(os.path.join(dir, aud_file)+'_audio.wav')"
      ],
      "metadata": {
        "id": "sVwdyGmEhoY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def background_foreground_separation(aud_file, segment):    # Using Spectral Subraction\n",
        "    audio, sr = librosa.load(aud_file)\n",
        "\n",
        "    # Foreground Separation\n",
        "\n",
        "    signal_spec = np.abs(librosa.stft(audio))\n",
        "    signal_power = np.square(signal_spec)\n",
        "\n",
        "    background_noise = audio[sr*segment[0]:sr*segment[1]]  # estimating background noise as a particular segment\n",
        "    background_noise_spec = np.abs(librosa.stft(background_noise))\n",
        "    background_noise_power = np.max(np.square(background_noise_spec), axis=1)\n",
        "\n",
        "    signal_power_denoised = np.maximum(signal_power - background_noise_power[:, np.newaxis], 0)   # subtracting background noise PSD from signal PSD\n",
        "    signal_spec_denoised = np.sqrt(signal_power_denoised) * np.exp(1j*np.angle(signal_spec))\n",
        "    audio_denoised = librosa.istft(signal_spec_denoised)\n",
        "\n",
        "    sf.write(aud_file+'_foreground.wav', audio_denoised, sr)\n",
        "\n",
        "    # Background Separation\n",
        "\n",
        "    background_noise_power = np.mean(np.square(background_noise_spec), axis=1)\n",
        "    signal_power_denoised = np.minimum(signal_power, 0.5*background_noise_power[:, np.newaxis])\n",
        "    signal_spec_denoised = np.sqrt(signal_power_denoised) * np.exp(1j*np.angle(signal_spec))\n",
        "    audio_denoised = librosa.istft(signal_spec_denoised)\n",
        "\n",
        "    sf.write(aud_file+'_background.wav', audio_denoised, sr)\n",
        "\n",
        "    # display\n",
        "\n",
        "    # print('Original Music - ' + aud_file.split('/')[-1])\n",
        "    # display(Audio(aud_file))\n",
        "    print('Foreground Music - ' + aud_file.split('/')[-1])\n",
        "    display(Audio(aud_file+'_foreground.wav'))\n",
        "    print('\\nBackground Music - ' + aud_file.split('/')[-1])\n",
        "    display(Audio(aud_file+'_background.wav'))\n"
      ],
      "metadata": {
        "id": "kDgglThqlxsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segments = {'Black Bloc' : [97,113], 'Clara Berry And Wooldog' : [79,85], 'James May' : [0,17], 'Titanium' : [0,38], 'Wall Of Death' : [0,12]}\n",
        "for aud_file in [aud_file for aud_file in os.listdir(dir) if aud_file.endswith('_audio.wav')]:\n",
        "    background_foreground_separation(os.path.join(dir,aud_file), segments[aud_file.split('-')[0].strip()])\n",
        "    print('\\n\\n')"
      ],
      "metadata": {
        "id": "LDqBTvhjfu6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTZF8RmUVx59"
      },
      "source": [
        "# Pre-Emphasis Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSJ7SoxEVxJ4"
      },
      "outputs": [],
      "source": [
        "processed_audio_data = []\n",
        "segment_data = []\n",
        "\n",
        "for audio, sr, audio_files in audio_data:\n",
        "\n",
        "    duration=10\n",
        "    n = random.randint(0, len(audio) - sr * duration)\n",
        "\n",
        "    y_eq = librosa.effects.preemphasis(audio[n:n + sr*duration], coef=0.97)\n",
        "    y_comp = np.tanh(y_eq)\n",
        "    y_norm = librosa.util.normalize(y_comp)\n",
        "\n",
        "    segment_data.append((audio[n:n + sr*duration], sr, audio_files))\n",
        "    processed_audio_data.append((y_norm, sr, audio_files))\n",
        "\n",
        "fig, axs = plt.subplots(2, 5, figsize=(20, 8))\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(segment_data):\n",
        "    # Original Audio\n",
        "    librosa.display.waveshow(audio, sr=sr, ax=axs[0, i])\n",
        "    axs[0, i].set_title(f'Original Audio\\n{audio_files}')\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(processed_audio_data):\n",
        "    # Enhanced Audio\n",
        "    librosa.display.waveshow(audio, sr=sr, ax=axs[1, i])\n",
        "    axs[1, i].set_title(f'Enhanced Audio\\n{audio_files}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sznH7aOpcZu6"
      },
      "source": [
        "# Butter High-Pass and Low-Pass Filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXXMeyBUcbSz"
      },
      "outputs": [],
      "source": [
        "def lowPassFilter(audio_signals, cutoff_freq, sample_rate, order=5):\n",
        "    nyquist = 0.5 * sample_rate\n",
        "    normal_cutoff = cutoff_freq / nyquist\n",
        "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "    filtered_data = lfilter(b, a, audio_signals)\n",
        "    return filtered_data\n",
        "\n",
        "def highPassFilter(audio_signals, cutoff_freq, sample_rate, order=5):\n",
        "    nyquist = 0.5 * sample_rate\n",
        "    normal_cutoff = cutoff_freq / nyquist\n",
        "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
        "    filtered_data = lfilter(b, a, audio_signals)\n",
        "    return filtered_data\n",
        "\n",
        "fig, axs = plt.subplots(3, 5, figsize=(20, 8))\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(audio_data):\n",
        "\n",
        "    duration=10\n",
        "    n = random.randint(0, len(audio) - sr * duration)\n",
        "\n",
        "    # Original\n",
        "    axs[0, i].set_title(f\"Original\\n{audio_files}\")\n",
        "    librosa.display.waveshow(audio[n:n + sr*duration], sr=sr, ax=axs[0, i])\n",
        "    axs[0, i].set_xlabel(\"Time\")\n",
        "    axs[0, i].set_ylabel(\"Amplitude\")\n",
        "\n",
        "    # High-pass Filtered\n",
        "    high_pass = highPassFilter(audio[n:n + sr*duration], 2000, sr)\n",
        "    axs[1, i].set_title(f\"High-pass\\n{audio_files}\")\n",
        "    librosa.display.waveshow(high_pass, sr=sr, ax=axs[1, i])\n",
        "    axs[1, i].set_xlabel(\"Time\")\n",
        "    axs[1, i].set_ylabel(\"Amplitude\")\n",
        "\n",
        "    # Low-pass Filtered\n",
        "    low_pass = lowPassFilter(audio[n:n + sr*duration], 500, sr)\n",
        "    axs[2, i].set_title(f\"Low-pass\\n{audio_files}\")\n",
        "    librosa.display.waveshow(low_pass, sr=sr, ax=axs[2, i])\n",
        "    axs[2, i].set_xlabel(\"Time\")\n",
        "    axs[2, i].set_ylabel(\"Amplitude\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaZr6myrg46S"
      },
      "source": [
        "# Bandpass Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLATsvtkhAsl"
      },
      "outputs": [],
      "source": [
        "def bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    filtered_data = lfilter(b, a, data)\n",
        "    return filtered_data\n",
        "\n",
        "fig, axs = plt.subplots(2, 5, figsize=(20, 4))\n",
        "\n",
        "lowcut = 80.0\n",
        "highcut = 1100.0\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(audio_data):\n",
        "\n",
        "    duration=10\n",
        "    n = random.randint(0, len(audio) - sr * duration)\n",
        "\n",
        "    axs[0, i].set_title(f\"Original\\n{audio_files}\")\n",
        "    librosa.display.waveshow(audio[n:n + sr*duration], sr=sr, ax=axs[0, i])\n",
        "    axs[0, i].set_xlabel(\"Time\")\n",
        "    axs[0, i].set_ylabel(\"Amplitude\")\n",
        "\n",
        "    filtered_audio = bandpass_filter(audio[n:n + sr*duration], lowcut, highcut, sr)\n",
        "    librosa.display.waveshow(filtered_audio, sr=sr, ax=axs[1, i])\n",
        "    axs[1, i].set_title(f\"Filtered Waveform\\n{audio_files}\")\n",
        "    axs[1, i].set_xlabel(\"Time\")\n",
        "    axs[1, i].set_ylabel(\"Amplitude\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bE_mvcBhwtj"
      },
      "source": [
        "# Notch Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iCvcbQmhvj5"
      },
      "outputs": [],
      "source": [
        "def notch_filter(data, notch_freq, fs, Q=30):\n",
        "    b, a = iirnotch(notch_freq / (0.5 * fs), Q)\n",
        "    filtered_data = lfilter(b, a, data)\n",
        "    return filtered_data\n",
        "\n",
        "fig, axs = plt.subplots(2, 5, figsize=(20, 4))\n",
        "\n",
        "notch_freq = 300.0\n",
        "Q = 30\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(audio_data):\n",
        "\n",
        "    duration=10\n",
        "    n = random.randint(0, len(audio) - sr * duration)\n",
        "\n",
        "    axs[0, i].set_title(f\"Original\\n{audio_files}\")\n",
        "    librosa.display.waveshow(audio[n:n + sr*duration], sr=sr, ax=axs[0, i])\n",
        "    axs[0, i].set_xlabel(\"Time\")\n",
        "    axs[0, i].set_ylabel(\"Amplitude\")\n",
        "\n",
        "    filtered_audio = notch_filter(audio[n:n + sr*duration], notch_freq, sr, Q)\n",
        "    librosa.display.waveshow(filtered_audio, sr=sr, ax=axs[1,i])\n",
        "    axs[1,i].set_title(f\"Notch Filtered Waveform\\n{audio_files}\")\n",
        "    axs[1,i].set_xlabel(\"Time\")\n",
        "    axs[1,i].set_ylabel(\"Amplitude\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnV0S4RhR5PA"
      },
      "source": [
        "# De-essing Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R67_CP0GR6TP"
      },
      "outputs": [],
      "source": [
        "def de_essing_filter(data, threshold=0.04, ratio=0.2):\n",
        "    essing_idx = data > threshold\n",
        "    data[essing_idx] *= ratio\n",
        "    return data\n",
        "\n",
        "fig, axs = plt.subplots(2, 5, figsize=(20, 4))\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(audio_data):\n",
        "\n",
        "    duration=10\n",
        "    n = random.randint(0, len(audio) - sr * duration)\n",
        "\n",
        "    axs[0, i].set_title(f\"Original\\n{audio_files}\")\n",
        "    librosa.display.waveshow(audio[n:n + sr*duration], sr=sr, ax=axs[0, i])\n",
        "    axs[0, i].set_xlabel(\"Time\")\n",
        "    axs[0, i].set_ylabel(\"Amplitude\")\n",
        "\n",
        "\n",
        "    filtered_audio = de_essing_filter(audio[n:n + sr*duration].copy())\n",
        "    librosa.display.waveshow(filtered_audio, sr=sr, ax=axs[1, i])\n",
        "    axs[1, i].set_title(f\"De-essed Waveform\\n{audio_files}\")\n",
        "    axs[1, i].set_xlabel(\"Time\")\n",
        "    axs[1, i].set_ylabel(\"Amplitude\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ylLHloSlz5"
      },
      "source": [
        "# Equalization Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la1T51pnSmw3"
      },
      "outputs": [],
      "source": [
        "def equalization_filter(data, freq, fs, gain, type='low', order=5):\n",
        "    b, a = butter(order, freq / (0.5 * fs), btype=type)\n",
        "    return gain * lfilter(b, a, data)\n",
        "\n",
        "fig, axs = plt.subplots(2, 5, figsize=(20, 4))\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(audio_data):\n",
        "\n",
        "    duration=10\n",
        "    n = random.randint(0, len(audio) - sr * duration)\n",
        "\n",
        "    axs[0, i].set_title(f\"Original\\n{audio_files}\")\n",
        "    librosa.display.waveshow(audio[n:n + sr*duration], sr=sr, ax=axs[0, i])\n",
        "    axs[0, i].set_xlabel(\"Time\")\n",
        "    axs[0, i].set_ylabel(\"Amplitude\")\n",
        "\n",
        "\n",
        "    filtered_audio = equalization_filter(audio[n:n + sr*duration].copy(), 1000, sr, 1.5, 'low')\n",
        "    librosa.display.waveshow(filtered_audio, sr=sr, ax=axs[1, i])\n",
        "    axs[1, i].set_title(f\"Equalized Waveform\\n{audio_files}\")\n",
        "    axs[1, i].set_xlabel(\"Time\")\n",
        "    axs[1, i].set_ylabel(\"Amplitude\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZYawhMCS0Ep"
      },
      "source": [
        "# Compressor Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgflW_yJS9Qg"
      },
      "outputs": [],
      "source": [
        "def compressor_filter(data, threshold=0.1, ratio=0.5):\n",
        "    idx = np.abs(data) > threshold\n",
        "    data[idx] = threshold + ratio * (data[idx] - threshold)\n",
        "    return data\n",
        "\n",
        "fig, axs = plt.subplots(2, 5, figsize=(20, 4))\n",
        "\n",
        "for i, (audio, sr, audio_files) in enumerate(audio_data):\n",
        "\n",
        "    duration=10\n",
        "    n = random.randint(0, len(audio) - sr * duration)\n",
        "\n",
        "    axs[0, i].set_title(f\"Original\\n{audio_files}\")\n",
        "    librosa.display.waveshow(audio[n:n + sr*duration], sr=sr, ax=axs[0, i])\n",
        "    axs[0, i].set_xlabel(\"Time\")\n",
        "    axs[0, i].set_ylabel(\"Amplitude\")\n",
        "\n",
        "    filtered_audio = compressor_filter(audio[n:n + sr*duration].copy())\n",
        "    librosa.display.waveshow(filtered_audio, sr=sr, ax=axs[1, i])\n",
        "    axs[1, i].set_title(f\"Compressed Waveform\\n{audio_files}\")\n",
        "    axs[1, i].set_xlabel(\"Time\")\n",
        "    axs[1, i].set_ylabel(\"Amplitude\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
